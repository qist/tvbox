# -*- coding: utf-8 -*-
# ğŸŒˆ Love 
import json
import random
import re
import sys
import threading
import time
from base64 import b64decode, b64encode
from urllib.parse import urlparse, quote

import requests
from pyquery import PyQuery as pq
sys.path.append('..')
from base.spider import Spider


class Spider(Spider):

    def init(self, extend=""):
        try:self.proxies = json.loads(extend)
        except:self.proxies = {}
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/134.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9',
            'Connection': 'keep-alive',
            'Cache-Control': 'no-cache',
        }
        # Use working dynamic URLs directly
        self.host = self.get_working_host()
        self.headers.update({'Origin': self.host, 'Referer': f"{self.host}/"})
        self.log(f"ä½¿ç”¨ç«™ç‚¹: {self.host}")
        print(f"ä½¿ç”¨ç«™ç‚¹: {self.host}")
        pass

    def getName(self):
        return "ğŸŒˆ ä»Šæ—¥çœ‹æ–™"

    def isVideoFormat(self, url):
        # Treat direct media formats as playable without parsing
        return any(ext in (url or '') for ext in ['.m3u8', '.mp4', '.ts'])

    def manualVideoCheck(self):
        return False

    def destroy(self):
        pass

    def homeContent(self, filter):
        try:
            response = requests.get(self.host, headers=self.headers, proxies=self.proxies, timeout=15)
            if response.status_code != 200:
                return {'class': [], 'list': []}
                
            data = self.getpq(response.text)
            result = {}
            classes = []
            
            # ä¼˜å…ˆä»å¯¼èˆªæ è·å–åˆ†ç±»
            nav_selectors = [
                '#navbarCollapse .navbar-nav .nav-item .nav-link',
                '.navbar-nav .nav-item .nav-link',
                '#nav .menu-item a',
                '.menu .menu-item a'
            ]
            
            found_categories = False
            for selector in nav_selectors:
                for item in data(selector).items():
                    href = item.attr('href') or ''
                    name = item.text().strip()
                    
                    # è¿‡æ»¤æ‰éåˆ†ç±»é“¾æ¥
                    if (not href or not name or 
                        href == '#' or 
                        href.startswith('http') or
                        'about' in href.lower() or
                        'contact' in href.lower() or
                        'tags' in href.lower() or
                        'top' in href.lower() or
                        'start' in href.lower() or
                        'time' in href.lower()):
                        continue
                    
                    # ç¡®ä¿æ˜¯åˆ†ç±»é“¾æ¥ï¼ˆåŒ…å«categoryæˆ–æ˜ç¡®çš„åˆ†ç±»è·¯å¾„ï¼‰
                    if '/category/' in href or any(cat in href for cat in ['/dy/', '/ks/', '/douyu/', '/hy/', '/hj/', '/tt/', '/wh/', '/asmr/', '/xb/', '/xsp/', '/rdgz/']):
                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                        if href.startswith('/'):
                            type_id = href
                        else:
                            type_id = f'/{href}'
                            
                        classes.append({
                            'type_name': name,
                            'type_id': type_id
                        })
                        found_categories = True
            
            # å¦‚æœå¯¼èˆªæ æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»åˆ†ç±»ä¸‹æ‹‰èœå•è·å–
            if not found_categories:
                category_selectors = [
                    '.category-list a',
                    '.slide-toggle + .category-list a',
                    '.menu .category-list a'
                ]
                for selector in category_selectors:
                    for item in data(selector).items():
                        href = item.attr('href') or ''
                        name = item.text().strip()
                        
                        if href and name and href != '#':
                            if href.startswith('/'):
                                type_id = href
                            else:
                                type_id = f'/{href}'
                                
                            classes.append({
                                'type_name': name,
                                'type_id': type_id
                            })
                            found_categories = True
            
            # å»é‡
            unique_classes = []
            seen_ids = set()
            for cls in classes:
                if cls['type_id'] not in seen_ids:
                    unique_classes.append(cls)
                    seen_ids.add(cls['type_id'])
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†ç±»ï¼Œåˆ›å»ºé»˜è®¤åˆ†ç±»
            if not unique_classes:
                unique_classes = [
                    {'type_name': 'çƒ­ç‚¹å…³æ³¨', 'type_id': '/category/rdgz/'},
                    {'type_name': 'æŠ–éŸ³', 'type_id': '/category/dy/'},
                    {'type_name': 'å¿«æ‰‹', 'type_id': '/category/ks/'},
                    {'type_name': 'æ–—é±¼', 'type_id': '/category/douyu/'},
                    {'type_name': 'è™ç‰™', 'type_id': '/category/hy/'},
                    {'type_name': 'èŠ±æ¤’', 'type_id': '/category/hj/'},
                    {'type_name': 'æ¨ç‰¹', 'type_id': '/category/tt/'},
                    {'type_name': 'ç½‘çº¢', 'type_id': '/category/wh/'},
                    {'type_name': 'ASMR', 'type_id': '/category/asmr/'},
                    {'type_name': 'Xæ’­', 'type_id': '/category/xb/'},
                    {'type_name': 'å°è§†é¢‘', 'type_id': '/category/xsp/'}
                ]
            
            result['class'] = unique_classes
            result['list'] = self.getlist(data('#index article a, #archive article a'))
            return result
            
        except Exception as e:
            print(f"homeContent error: {e}")
            return {'class': [], 'list': []}

    def homeVideoContent(self):
        try:
            response = requests.get(self.host, headers=self.headers, proxies=self.proxies, timeout=15)
            if response.status_code != 200:
                return {'list': []}
            data = self.getpq(response.text)
            return {'list': self.getlist(data('#index article a, #archive article a'))}
        except Exception as e:
            print(f"homeVideoContent error: {e}")
            return {'list': []}

    def categoryContent(self, tid, pg, filter, extend):
        try:
            # ä¿®å¤URLæ„å»º - å»é™¤å¤šä½™çš„æ–œæ 
            base_url = tid.lstrip('/').rstrip('/')
            if pg and pg != '1':
                url = f"{self.host}{base_url}/{pg}/"
            else:
                url = f"{self.host}{base_url}/"
                
            print(f"åˆ†ç±»é¡µé¢URL: {url}")
            
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)
            if response.status_code != 200:
                print(f"åˆ†ç±»é¡µé¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return {'list': [], 'page': pg, 'pagecount': 1, 'limit': 90, 'total': 0}
                
            data = self.getpq(response.text)
            videos = self.getlist(data('#archive article a, #index article a, .post-card'), tid)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è§†é¢‘ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not videos:
                videos = self.getlist(data('article a, .post a, .entry-title a'), tid)
            
            print(f"æ‰¾åˆ° {len(videos)} ä¸ªè§†é¢‘")
            
            # æ”¹è¿›çš„é¡µæ•°æ£€æµ‹é€»è¾‘
            pagecount = self.detect_page_count(data, pg)
            
            result = {}
            result['list'] = videos
            result['page'] = pg
            result['pagecount'] = pagecount
            result['limit'] = 90
            result['total'] = 999999
            return result
            
        except Exception as e:
            print(f"categoryContent error: {e}")
            return {'list': [], 'page': pg, 'pagecount': 1, 'limit': 90, 'total': 0}

    def tagContent(self, tid, pg, filter, extend):
        """æ ‡ç­¾é¡µé¢å†…å®¹"""
        try:
            # ä¿®å¤URLæ„å»º - å»é™¤å¤šä½™çš„æ–œæ 
            base_url = tid.lstrip('/').rstrip('/')
            if pg and pg != '1':
                url = f"{self.host}{base_url}/{pg}/"
            else:
                url = f"{self.host}{base_url}/"
                
            print(f"æ ‡ç­¾é¡µé¢URL: {url}")
            
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)
            if response.status_code != 200:
                print(f"æ ‡ç­¾é¡µé¢è¯·æ±‚å¤±è´¥: {response.status_code}")
                return {'list': [], 'page': pg, 'pagecount': 1, 'limit': 90, 'total': 0}
                
            data = self.getpq(response.text)
            videos = self.getlist(data('#archive article a, #index article a, .post-card'), tid)
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è§†é¢‘ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
            if not videos:
                videos = self.getlist(data('article a, .post a, .entry-title a'), tid)
            
            print(f"æ‰¾åˆ° {len(videos)} ä¸ªæ ‡ç­¾ç›¸å…³è§†é¢‘")
            
            # é¡µæ•°æ£€æµ‹
            pagecount = self.detect_page_count(data, pg)
            
            result = {}
            result['list'] = videos
            result['page'] = pg
            result['pagecount'] = pagecount
            result['limit'] = 90
            result['total'] = 999999
            return result
            
        except Exception as e:
            print(f"tagContent error: {e}")
            return {'list': [], 'page': pg, 'pagecount': 1, 'limit': 90, 'total': 0}

    def detect_page_count(self, data, current_page):
        """æ”¹è¿›çš„é¡µæ•°æ£€æµ‹æ–¹æ³•"""
        pagecount = 99999  # é»˜è®¤å¤§æ•°å­—ï¼Œå…è®¸æ— é™ç¿»é¡µ
        
        # æ–¹æ³•1: æ£€æŸ¥åˆ†é¡µå™¨ä¸­çš„æ‰€æœ‰é¡µç é“¾æ¥
        page_numbers = []
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„é¡µç é“¾æ¥
        page_selectors = [
            '.page-navigator a',
            '.pagination a', 
            '.pages a',
            '.page-numbers a'
        ]
        
        for selector in page_selectors:
            for page_link in data(selector).items():
                href = page_link.attr('href') or ''
                text = page_link.text().strip()
                
                # ä»hrefä¸­æå–é¡µç 
                if href:
                    # åŒ¹é… /category/dy/2/ è¿™ç§æ ¼å¼
                    match = re.search(r'/(\d+)/?$', href.rstrip('/'))
                    if match:
                        page_num = int(match.group(1))
                        if page_num not in page_numbers:
                            page_numbers.append(page_num)
                
                # ä»æ–‡æœ¬ä¸­æå–æ•°å­—é¡µç 
                if text and text.isdigit():
                    page_num = int(text)
                    if page_num not in page_numbers:
                        page_numbers.append(page_num)
        
        # å¦‚æœæœ‰æ‰¾åˆ°é¡µç ï¼Œå–æœ€å¤§å€¼
        if page_numbers:
            max_page = max(page_numbers)
            print(f"ä»åˆ†é¡µå™¨æ£€æµ‹åˆ°æœ€å¤§é¡µç : {max_page}")
            return max_page
        
        # æ–¹æ³•2: æ£€æŸ¥æ˜¯å¦å­˜åœ¨"ä¸‹ä¸€é¡µ"æŒ‰é’®
        next_selectors = [
            '.page-navigator .next',
            '.pagination .next',
            '.next-page',
            'a:contains("ä¸‹ä¸€é¡µ")'
        ]
        
        for selector in next_selectors:
            if data(selector):
                print("æ£€æµ‹åˆ°ä¸‹ä¸€é¡µæŒ‰é’®ï¼Œå…è®¸ç»§ç»­ç¿»é¡µ")
                return 99999
        
        # æ–¹æ³•3: å¦‚æœå½“å‰é¡µè§†é¢‘æ•°é‡å¾ˆå°‘ï¼Œå¯èƒ½æ²¡æœ‰ä¸‹ä¸€é¡µ
        if len(data('#archive article, #index article, .post-card')) < 5:
            print("å½“å‰é¡µå†…å®¹è¾ƒå°‘ï¼Œå¯èƒ½æ²¡æœ‰ä¸‹ä¸€é¡µ")
            return int(current_page)
        
        print("ä½¿ç”¨é»˜è®¤é¡µæ•°: 99999")
        return 99999

    def detailContent(self, ids):
        try:
            url = f"{self.host}{ids[0]}" if not ids[0].startswith('http') else ids[0]
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)
            
            if response.status_code != 200:
                return {'list': [{'vod_play_from': 'ä»Šæ—¥çœ‹æ–™', 'vod_play_url': f'é¡µé¢åŠ è½½å¤±è´¥${url}'}]}
                
            data = self.getpq(response.text)
            vod = {'vod_play_from': 'ä»Šæ—¥çœ‹æ–™'}
            
            # è·å–æ ‡é¢˜
            title_selectors = ['.post-title', 'h1.entry-title', 'h1', '.post-card-title']
            for selector in title_selectors:
                title_elem = data(selector)
                if title_elem:
                    vod['vod_name'] = title_elem.text().strip()
                    break
            
            if 'vod_name' not in vod:
                vod['vod_name'] = 'ä»Šæ—¥çœ‹æ–™è§†é¢‘'
            
            # è·å–å†…å®¹/æè¿°
            try:
                clist = []
                if data('.tags .keywords a'):
                    for k in data('.tags .keywords a').items():
                        title = k.text()
                        href = k.attr('href')
                        if title and href:
                            # ä½¿hrefç›¸å¯¹è·¯å¾„
                            if href.startswith(self.host):
                                href = href.replace(self.host, '')
                            clist.append('[a=cr:' + json.dumps({'id': href, 'name': title}) + '/]' + title + '[/a]')
                vod['vod_content'] = ' '.join(clist) if clist else data('.post-content').text() or vod['vod_name']
            except:
                vod['vod_content'] = vod['vod_name']
            
            # è·å–è§†é¢‘URLs
            try:
                plist = []
                used_names = set()
                
                # æŸ¥æ‰¾DPlayerè§†é¢‘
                if data('.dplayer'):
                    for c, k in enumerate(data('.dplayer').items(), start=1):
                        config_attr = k.attr('data-config')
                        if config_attr:
                            try:
                                config = json.loads(config_attr)
                                video_url = config.get('video', {}).get('url', '')
                                if video_url:
                                    name = f"è§†é¢‘{c}"
                                    count = 2
                                    while name in used_names:
                                        name = f"è§†é¢‘{c}_{count}"
                                        count += 1
                                    used_names.add(name)
                                    self.log(f"è§£æåˆ°è§†é¢‘: {name} -> {video_url}")
                                    print(f"è§£æåˆ°è§†é¢‘: {name} -> {video_url}")
                                    plist.append(f"{name}${video_url}")
                            except:
                                continue
                
                # æŸ¥æ‰¾è§†é¢‘æ ‡ç­¾
                if not plist:
                    video_selectors = ['video source', 'video', 'iframe[src*="video"]', 'a[href*=".m3u8"]', 'a[href*=".mp4"]']
                    for selector in video_selectors:
                        for c, elem in enumerate(data(selector).items(), start=1):
                            src = elem.attr('src') or elem.attr('href') or ''
                            if src and any(ext in src for ext in ['.m3u8', '.mp4', 'video']):
                                name = f"è§†é¢‘{c}"
                                count = 2
                                while name in used_names:
                                    name = f"è§†é¢‘{c}_{count}"
                                    count += 1
                                used_names.add(name)
                                plist.append(f"{name}${src}")
                
                if plist:
                    self.log(f"æ‹¼è£…æ’­æ”¾åˆ—è¡¨ï¼Œå…±{len(plist)}ä¸ª")
                    print(f"æ‹¼è£…æ’­æ”¾åˆ—è¡¨ï¼Œå…±{len(plist)}ä¸ª")
                    vod['vod_play_url'] = '#'.join(plist)
                else:
                    vod['vod_play_url'] = f"æ­£ç‰‡${url}"
                    
            except Exception as e:
                print(f"è§†é¢‘è§£æé”™è¯¯: {e}")
                vod['vod_play_url'] = f"æ­£ç‰‡${url}"
                
            return {'list': [vod]}
            
        except Exception as e:
            print(f"detailContent error: {e}")
            return {'list': [{'vod_play_from': 'ä»Šæ—¥çœ‹æ–™', 'vod_play_url': f'è¯¦æƒ…é¡µåŠ è½½å¤±è´¥${ids[0] if ids else ""}'}]}

    def searchContent(self, key, quick, pg="1"):
        try:
            # ä¼˜å…ˆä½¿ç”¨æ ‡ç­¾æœç´¢
            encoded_key = quote(key)
            url = f"{self.host}/tag/{encoded_key}/{pg}" if pg != "1" else f"{self.host}/tag/{encoded_key}/"
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)
            
            if response.status_code != 200:
                # å°è¯•æœç´¢é¡µé¢
                url = f"{self.host}/search/{encoded_key}/{pg}" if pg != "1" else f"{self.host}/search/{encoded_key}/"
                response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)
            
            if response.status_code != 200:
                return {'list': [], 'page': pg}
                
            data = self.getpq(response.text)
            videos = self.getlist(data('#archive article a, #index article a, .post-card'))
            
            # ä½¿ç”¨æ”¹è¿›çš„é¡µæ•°æ£€æµ‹æ–¹æ³•
            pagecount = self.detect_page_count(data, pg)
            
            return {'list': videos, 'page': pg, 'pagecount': pagecount}
            
        except Exception as e:
            print(f"searchContent error: {e}")
            return {'list': [], 'page': pg}

    def getTagsContent(self, pg="1"):
        """è·å–æ ‡ç­¾é¡µé¢å†…å®¹"""
        try:
            url = f"{self.host}/tags.html"
            response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=15)
            
            if response.status_code != 200:
                return {'list': [], 'page': pg}
                
            data = self.getpq(response.text)
            tags = []
            
            # ä»æ ‡ç­¾é¡µé¢æå–æ‰€æœ‰æ ‡ç­¾ - ä½¿ç”¨æ›´å®½æ¾çš„é€‰æ‹©å™¨
            for tag_elem in data('a[href*="/tag/"]').items():
                tag_name = tag_elem.text().strip()
                tag_href = tag_elem.attr('href') or ''
                
                if tag_name and tag_href and '/tag/' in tag_href and tag_name != 'å…¨éƒ¨æ ‡ç­¾':  # æ’é™¤æ ‡é¢˜é“¾æ¥
                    # å¤„ç†ä¸ºç›¸å¯¹è·¯å¾„
                    tag_id = tag_href.replace(self.host, '')
                    if not tag_id.startswith('/'):
                        tag_id = '/' + tag_id
                    
                    tags.append({
                        'vod_id': tag_id,
                        'vod_name': f"ğŸ·ï¸ {tag_name}",
                        'vod_pic': '',
                        'vod_remarks': 'æ ‡ç­¾',
                        'vod_tag': 'tag',
                        'style': {"type": "rect", "ratio": 1.33}
                    })
            
            print(f"æ‰¾åˆ° {len(tags)} ä¸ªæ ‡ç­¾")
            
            # åˆ†é¡µå¤„ç† - æ ‡ç­¾é¡µé¢é€šå¸¸ä¸éœ€è¦åˆ†é¡µ
            result = {}
            result['list'] = tags
            result['page'] = pg
            result['pagecount'] = 1  # æ ‡ç­¾é¡µé¢é€šå¸¸åªæœ‰ä¸€é¡µ
            result['limit'] = 999
            result['total'] = len(tags)
            return result
            
        except Exception as e:
            print(f"getTagsContent error: {e}")
            return {'list': [], 'page': pg}

    def playerContent(self, flag, id, vipFlags):
        url = id
        p = 1
        if self.isVideoFormat(url):
            if '.m3u8' in url:
                url = self.proxy(url)
            p = 0
        self.log(f"æ’­æ”¾è¯·æ±‚: parse={p}, url={url}")
        print(f"æ’­æ”¾è¯·æ±‚: parse={p}, url={url}")
        return {'parse': p, 'url': url, 'header': self.headers}

    def localProxy(self, param):
        try:
            if param.get('type') == 'img':
                img_url = self.d64(param['url'])
                if not img_url.startswith(('http://', 'https://')):
                    if img_url.startswith('/'):
                        img_url = f"{self.host}{img_url}"
                    else:
                        img_url = f"{self.host}/{img_url}"
                
                res = requests.get(img_url, headers=self.headers, proxies=self.proxies, timeout=10)
                return [200, res.headers.get('Content-Type', 'image/jpeg'), res.content]
            elif param.get('type') == 'm3u8':
                return self.m3Proxy(param['url'])
            else:
                return self.tsProxy(param['url'])
        except Exception as e:
            print(f"localProxy error: {e}")
            return [500, "text/plain", f"Proxy error: {str(e)}".encode()]

    def proxy(self, data, type='m3u8'):
        if data and len(self.proxies):
            return f"{self.getProxyUrl()}&url={self.e64(data)}&type={type}"
        else:
            return data

    def m3Proxy(self, url):
        try:
            url = self.d64(url)
            ydata = requests.get(url, headers=self.headers, proxies=self.proxies, allow_redirects=False)
            data = ydata.content.decode('utf-8')
            if ydata.headers.get('Location'):
                url = ydata.headers['Location']
                data = requests.get(url, headers=self.headers, proxies=self.proxies).content.decode('utf-8')
            lines = data.strip().split('\n')
            last_r = url[:url.rfind('/')]
            parsed_url = urlparse(url)
            durl = parsed_url.scheme + "://" + parsed_url.netloc
            iskey = True
            for index, string in enumerate(lines):
                if iskey and 'URI' in string:
                    pattern = r'URI="([^"]*)"'
                    match = re.search(pattern, string)
                    if match:
                        lines[index] = re.sub(pattern, f'URI="{self.proxy(match.group(1), "mkey")}"', string)
                        iskey = False
                        continue
                if '#EXT' not in string:
                    if 'http' not in string:
                        domain = last_r if string.count('/') < 2 else durl
                        string = domain + ('' if string.startswith('/') else '/') + string
                    lines[index] = self.proxy(string, string.split('.')[-1].split('?')[0])
            data = '\n'.join(lines)
            return [200, "application/vnd.apple.mpegur", data]
        except Exception as e:
            print(f"m3Proxy error: {e}")
            return [500, "text/plain", f"m3u8 proxy error: {str(e)}".encode()]

    def tsProxy(self, url):
        try:
            url = self.d64(url)
            data = requests.get(url, headers=self.headers, proxies=self.proxies, stream=True)
            return [200, data.headers.get('Content-Type', 'video/mp2t'), data.content]
        except Exception as e:
            print(f"tsProxy error: {e}")
            return [500, "text/plain", f"ts proxy error: {str(e)}".encode()]

    def e64(self, text):
        try:
            text_bytes = text.encode('utf-8')
            encoded_bytes = b64encode(text_bytes)
            return encoded_bytes.decode('utf-8')
        except Exception as e:
            print(f"Base64ç¼–ç é”™è¯¯: {str(e)}")
            return ""

    def d64(self, encoded_text):
        try:
            encoded_bytes = encoded_text.encode('utf-8')
            decoded_bytes = b64decode(encoded_bytes)
            return decoded_bytes.decode('utf-8')
        except Exception as e:
            print(f"Base64è§£ç é”™è¯¯: {str(e)}")
            return ""

    def get_working_host(self):
        """Get working host from known dynamic URLs"""
        dynamic_urls = [
            'https://kanliao2.one/',
            'https://kanliao7.org/',
            'https://kanliao7.net/',
            'https://kanliao14.com/'
        ]
        
        for url in dynamic_urls:
            try:
                response = requests.get(url, headers=self.headers, proxies=self.proxies, timeout=10)
                if response.status_code == 200:
                    data = self.getpq(response.text)
                    articles = data('#index article a, #archive article a')
                    if len(articles) > 0:
                        self.log(f"é€‰ç”¨å¯ç”¨ç«™ç‚¹: {url}")
                        print(f"é€‰ç”¨å¯ç”¨ç«™ç‚¹: {url}")
                        return url
            except Exception as e:
                continue
        
        self.log(f"æœªæ£€æµ‹åˆ°å¯ç”¨ç«™ç‚¹ï¼Œå›é€€: {dynamic_urls[0]}")
        print(f"æœªæ£€æµ‹åˆ°å¯ç”¨ç«™ç‚¹ï¼Œå›é€€: {dynamic_urls[0]}")
        return dynamic_urls[0]

    def getlist(self, data, tid=''):
        videos = []
        for k in data.items():
            a = k.attr('href')
            b = k('h2').text() or k('.post-card-title').text() or k('.entry-title').text() or k.text()
            c = k('span[itemprop="datePublished"]').text() or k('.post-meta, .entry-meta, time, .post-card-info').text()
            
            # è¿‡æ»¤å¹¿å‘Šï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«"çƒ­æœHOT"æ ‡å¿—
            if self.is_advertisement(k):
                print(f"è¿‡æ»¤å¹¿å‘Š: {b}")
                continue
                
            if a and b and b.strip():
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if not a.startswith('http'):
                    if a.startswith('/'):
                        vod_id = a
                    else:
                        vod_id = f'/{a}'
                else:
                    vod_id = a
                    
                videos.append({
                    'vod_id': vod_id,
                    'vod_name': b.replace('\n', ' ').strip(),
                    'vod_pic': self.get_article_img(k),
                    'vod_remarks': c.strip() if c else '',
                    'vod_tag': '',
                    'style': {"type": "rect", "ratio": 1.33}
                })
        return videos

    def is_advertisement(self, article_elem):
        """åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Šï¼ˆåŒ…å«çƒ­æœHOTæ ‡å¿—ï¼‰"""
        # æ£€æŸ¥.wrapså…ƒç´ æ˜¯å¦åŒ…å«"çƒ­æœHOT"æ–‡æœ¬
        hot_elements = article_elem.find('.wraps')
        for elem in hot_elements.items():
            if 'çƒ­æœHOT' in elem.text():
                return True
        
        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦åŒ…å«å¹¿å‘Šå…³é”®è¯
        title = article_elem('h2').text() or article_elem('.post-card-title').text() or ''
        ad_keywords = ['çƒ­æœHOT', 'æ‰‹æœºé“¾æ¥', 'DNSè®¾ç½®', 'ä¿®æ”¹DNS', 'WIFIè®¾ç½®']
        if any(keyword in title for keyword in ad_keywords):
            return True
            
        # æ£€æŸ¥èƒŒæ™¯é¢œè‰²æ˜¯å¦ä¸ºå¹¿å‘Šç‰¹æœ‰çš„æ¸å˜èƒŒæ™¯
        style = article_elem.attr('style') or ''
        if 'background:' in style and any(gradient in style for gradient in ['-webkit-linear-gradient', 'linear-gradient']):
            # è¿›ä¸€æ­¥æ£€æŸ¥æ˜¯å¦åŒ…å«ç‰¹å®šçš„å¹¿å‘Šé¢œè‰²ç»„åˆ
            ad_gradients = ['#ec008c,#fc6767', '#ffe259,#ffa751']
            if any(gradient in style for gradient in ad_gradients):
                return True
                
        return False

    def get_article_img(self, article_elem):
        """ä»æ–‡ç« å…ƒç´ ä¸­æå–å›¾ç‰‡ï¼Œå¤šç§æ–¹å¼å°è¯•"""
        # æ–¹å¼1: ä»scriptæ ‡ç­¾ä¸­æå–loadBannerDirect
        script_text = article_elem('script').text()
        if script_text:
            match = re.search(r"loadBannerDirect\('([^']+)'", script_text)
            if match:
                url = match.group(1)
                if not url.startswith(('http://', 'https://')):
                    if url.startswith('/'):
                        url = f"{self.host}{url}"
                    else:
                        url = f"{self.host}/{url}"
                return f"{self.getProxyUrl()}&url={self.e64(url)}&type=img"
        
        # æ–¹å¼2: ä»èƒŒæ™¯å›¾ç‰‡ä¸­æå–
        bg_elem = article_elem.find('.blog-background')
        if bg_elem:
            style = bg_elem.attr('style') or ''
            bg_match = re.search(r'background-image:\s*url\(["\']?([^"\'\)]+)["\']?\)', style)
            if bg_match:
                img_url = bg_match.group(1)
                if img_url and not img_url.startswith('data:'):
                    if not img_url.startswith(('http://', 'https://')):
                        if img_url.startswith('/'):
                            img_url = f"{self.host}{img_url}"
                        else:
                            img_url = f"{self.host}/{img_url}"
                    return f"{self.getProxyUrl()}&url={self.e64(img_url)}&type=img"
        
        # æ–¹å¼3: ä»å›¾ç‰‡æ ‡ç­¾ä¸­æå–
        img_elem = article_elem.find('img')
        if img_elem:
            data_src = img_elem.attr('data-src')
            if data_src:
                if not data_src.startswith(('http://', 'https://')):
                    if data_src.startswith('/'):
                        data_src = f"{self.host}{data_src}"
                    else:
                        data_src = f"{self.host}/{data_src}"
                return f"{self.getProxyUrl()}&url={self.e64(data_src)}&type=img"
            
            src = img_elem.attr('src')
            if src:
                if not src.startswith(('http://', 'https://')):
                    if src.startswith('/'):
                        src = f"{self.host}{src}"
                    else:
                        src = f"{self.host}/{src}"
                return f"{self.getProxyUrl()}&url={self.e64(src)}&type=img"
        
        return ''

    def getpq(self, data):
        try:
            return pq(data)
        except Exception as e:
            print(f"{str(e)}")
            return pq(data.encode('utf-8'))