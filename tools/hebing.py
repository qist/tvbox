import json
import hashlib
import re
import sys
import os
# è®¡ç®—æœ¬åœ°æ–‡ä»¶ fan.txt çš„ md5
def get_md5(filepath):
    md5 = hashlib.md5()
    with open(filepath, "rb") as f:
        while chunk := f.read(8192):
            md5.update(chunk)
    return md5.hexdigest()

# åŠ è½½ JSON æ–‡ä»¶
def load_json(path):
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)

# ä¿å­˜ JSON æ–‡ä»¶ï¼ˆæŠ˜å å­—å…¸æ•°ç»„ä¸ºå•è¡Œï¼Œç©ºæ•°ç»„å’ŒåŸºç¡€æ•°ç»„ä¸€è¡Œï¼‰
class CompactJSONEncoder(json.JSONEncoder):
    def iterencode(self, o, _one_shot=False):
        def _compact_list(lst, indent_level):
            pad = '  ' * indent_level
            if not lst or all(isinstance(i, (str, int, float, bool, type(None))) for i in lst):
                return json.dumps(lst, ensure_ascii=False)
            if all(isinstance(i, dict) for i in lst):
                return '[\n' + ',\n'.join([pad + '  ' + json.dumps(i, ensure_ascii=False, separators=(',', ': ')) for i in lst]) + '\n' + pad + ']'
            return json.dumps(lst, ensure_ascii=False, indent=2)

        def _encode(obj, indent_level=0):
            pad = '  ' * indent_level
            if isinstance(obj, dict):
                lines = [f'"{k}": {_encode(v, indent_level+1)}' for k, v in obj.items()]
                return '{\n' + pad + '  ' + (',\n' + pad + '  ').join(lines) + '\n' + pad + '}'
            elif isinstance(obj, list):
                return _compact_list(obj, indent_level)
            return json.dumps(obj, ensure_ascii=False)

        return iter([_encode(o)])

def save_json(data, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False, cls=CompactJSONEncoder)
    print(f"âœ… å·²ä¿å­˜ï¼š{path}")

# æ’å…¥ cleaned_sites åˆ°ç›®æ ‡ sites ä¸­çš„ç›®æ ‡æ¡ç›®ä¹‹ä¸Š
def insert_sites(base_sites, cleaned_sites, key_marker="å¥‡ä¼˜"):
    for i, item in enumerate(base_sites):
        if item.get("key") == key_marker:
            return base_sites[:i] + cleaned_sites + base_sites[i:]
    print(f"âš ï¸ æœªæ‰¾åˆ° key ä¸º {key_marker} çš„æ’å…¥ç‚¹ï¼Œè¿½åŠ åˆ°æœ«å°¾")
    return base_sites + cleaned_sites

if __name__ == "__main__":
    # é»˜è®¤è·¯å¾„
    dianshi_path = "dianshi.json"
    cleaned_path = "tvbox_cleaned.json"

    # è¦†ç›–é»˜è®¤è·¯å¾„ï¼ˆå¦‚æœä¼ äº†å‚æ•°ï¼‰
    if len(sys.argv) > 1:
        dianshi_path = sys.argv[1]
    if len(sys.argv) > 2:
        cleaned_path = sys.argv[2]

    try:
        # è·å– fan.txt çš„ md5
        md5_value = get_md5("fan.txt")
        print(f"ğŸ” fan.txt çš„ MD5: {md5_value}")

        # åŠ è½½ä¸¤ä¸ª JSON æ–‡ä»¶
        dianshi = load_json(dianshi_path)
        cleaned = load_json(cleaned_path)

        # æ›¿æ¢ spider md5
        if "spider" in dianshi:
            old_spider = dianshi["spider"]
            new_spider = re.sub(r'md5;[a-f0-9]+', f'md5;{md5_value}', old_spider)
            dianshi["spider"] = new_spider
            print(f"ğŸ”„ æ›¿æ¢ spider å­—æ®µä¸º: {new_spider}")
        else:
            print("âš ï¸ dianshi.json ä¸­æœªæ‰¾åˆ° spider å­—æ®µ")

        # æ’å…¥ sites
        cleaned_sites = cleaned.get("sites", [])
        dianshi["sites"] = insert_sites(dianshi.get("sites", []), cleaned_sites)
        name, ext = os.path.splitext(dianshi_path)
        output_path = f"{name}_merged{ext}"

        save_json(dianshi, output_path)
        # ä¿å­˜æœ€ç»ˆåˆå¹¶æ–‡ä»¶
        # save_json(dianshi, "dianshi_merged.json")

    except Exception as e:
        print(f"âŒ å‡ºé”™: {e}")

